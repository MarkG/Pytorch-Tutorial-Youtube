{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MarkG/my-ml-practices/blob/main/yelp_classifier_task.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task: build a binary classifier based on the Yelp dataset and show your results"
      ],
      "metadata": {
        "id": "Dz38AR2wj7aZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step one: plan how to solve this task.\n",
        "\n",
        "1.   Candidate to fill in this part\n",
        "\n"
      ],
      "metadata": {
        "id": "6Ig-NCtYkas0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title basic imports\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from typing import Tuple\n",
        "from google.colab import drive\n",
        "\n",
        "# url to file: https://drive.google.com/file/d/1-4gpC-f3CsTjGNfaRhxKz2FmYQXpNQBn/view\n",
        "# use drive or upload to contents dir\n",
        "#drive.mount(\"/content/drive\")\n",
        "filepath = \"yelp_labelled.txt\""
      ],
      "metadata": {
        "id": "wTqDxgrNXkWl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title load data\n",
        "\n",
        "def load_file_values(filepath: str) -> Tuple[np.ndarray, np.ndarray]:\n",
        "  df = pd.read_csv(filepath, names=['sentence', 'label'], sep='\\t')\n",
        "  sentences = df['sentence'].values\n",
        "  labels = df['label'].values\n",
        "  return sentences, labels"
      ],
      "metadata": {
        "id": "S7hb1GPjlefZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title split data and preprocess it\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def preprocess_and_split_data(sentences: np.ndarray, labels: np.ndarray):\n",
        "  X_train, X_test, y_train, y_test = train_test_split(sentences, labels, test_size=0.2, random_state=42)\n",
        "  return X_train, X_test, y_train, y_test"
      ],
      "metadata": {
        "id": "0tub8EElRZzG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "sklearn train_test_split syntax: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html"
      ],
      "metadata": {
        "id": "H5aaBSwLSMub"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer # Import TF-IDF\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from typing import Tuple\n",
        "\n",
        "def load_file_values(filepath: str) -> Tuple[np.ndarray, np.ndarray]:\n",
        "  df = pd.read_csv(filepath, names=['sentence', 'label'], sep='\\t')\n",
        "  sentences = df['sentence'].values\n",
        "  labels = df['label'].values\n",
        "  return sentences, labels\n",
        "\n",
        "sentences, labels = load_file_values(filepath)\n",
        "\n",
        "# 1. Create a TF-IDF vectorizer\n",
        "vectorizer = TfidfVectorizer()\n",
        "\n",
        "# 2. Fit the vectorizer to the entire dataset and transform it\n",
        "all_data_vec = vectorizer.fit_transform(sentences)\n",
        "\n",
        "# 3. Split the vectorized data into training and testing sets\n",
        "X_train_vec, X_test_vec, y_train, y_test = train_test_split(all_data_vec, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# 4. Create and train the Logistic Regression model\n",
        "lr = LogisticRegression()\n",
        "lr.fit(X_train_vec, y_train)\n",
        "\n",
        "# 5. Make predictions on the test set\n",
        "y_pred = lr.predict(X_test_vec)\n",
        "\n",
        "# 6. Evaluate the model's accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy}\")"
      ],
      "metadata": {
        "id": "xRXoq06OjHqu",
        "outputId": "63f00677-32d5-4601-a35a-ca6bb83b6f3e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.81\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "how to inprove accurecy?\n",
        "\n",
        "# Gemini:\n",
        "Okay, here are some ways to improve the accuracy of your binary classifier:\n",
        "\n",
        "1. Hyperparameter Tuning:\n",
        "\n",
        "Reasoning: The default parameters of the Logistic Regression model might not be optimal for your dataset. By tuning hyperparameters like C (regularization strength) and penalty (type of regularization), you can potentially find a better configuration that improves accuracy.\n",
        "Code:\n",
        "\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "```\n",
        "    # Define the parameter grid to search\n",
        "    param_grid = {'C': [0.1, 1, 10], 'penalty': ['l1', 'l2']}\n",
        "\n",
        "    # Create a GridSearchCV object\n",
        "    grid_search = GridSearchCV(lr, param_grid, cv=5)  # cv=5 for 5-fold cross-validation\n",
        "\n",
        "    # Fit the GridSearchCV object to the training data\n",
        "    grid_search.fit(X_train_vec, y_train)\n",
        "\n",
        "    # Get the best hyperparameters\n",
        "    best_params = grid_search.best_params_\n",
        "\n",
        "    # Create a new Logistic Regression model with the best hyperparameters\n",
        "    best_lr = LogisticRegression(**best_params)\n",
        "\n",
        "    # Train the new model\n",
        "    best_lr.fit(X_train_vec, y_train)\n",
        "\n",
        "    # Make predictions and evaluate accuracy\n",
        "    y_pred = best_lr.predict(X_test_vec)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    print(f\"Accuracy with best hyperparameters: {accuracy}\")\n",
        "```\n",
        "Use code with caution\n",
        "2. Feature Engineering:\n",
        "\n",
        "Reasoning: The current features (TF-IDF) might not capture all the relevant information in the text. You can create new features or transform existing ones to improve the model's ability to learn meaningful patterns.\n",
        "Example: Consider adding features like sentiment scores, the presence of specific keywords, or the length of the sentences.\n",
        "3. Different Algorithm:\n",
        "\n",
        "Reasoning: Logistic Regression might not be the best algorithm for your specific dataset. Try using other classification algorithms like Support Vector Machines (SVM), Random Forest, or Naive Bayes.\n",
        "Code Example (using SVM):\n",
        "\n",
        "from sklearn.svm import SVC\n",
        "```\n",
        "    # Create an SVM classifier\n",
        "    svm = SVC()\n",
        "\n",
        "    # Train the SVM model\n",
        "    svm.fit(X_train_vec, y_train)\n",
        "\n",
        "    # Make predictions and evaluate accuracy\n",
        "    y_pred = svm.predict(X_test_vec)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    print(f\"Accuracy with SVM: {accuracy}\")\n",
        "```\n",
        "Use code with caution\n",
        "4. More data\n",
        "\n",
        "Reasoning: More training data can often lead to better model performance.\n",
        "Implementation: Try to collect more labeled data if possible.\n",
        "5. Data Augmentation\n",
        "\n",
        "Reasoning: If collecting more data is difficult, you can use data augmentation techniques to create synthetic variations of your existing data.\n",
        "Example: For text data, you can replace words with synonyms or randomly insert or delete words.\n",
        "By applying these suggestions, you can likely improve the accuracy of your binary classifier. Choose a suggestion and let me know if you want me to help you apply it to your code."
      ],
      "metadata": {
        "id": "IzX03dQUQbZb"
      }
    }
  ]
}